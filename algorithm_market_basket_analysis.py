# -*- coding: utf-8 -*-
"""Algorithm-Market-Basket-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O7vF7q-jccreshwVprW9Me2zFK9XUKLx

# DATA LOADING
"""

!pip install pyspark

import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MarketBasketAnalysis").getOrCreate()

df_1 = spark.read.csv("/content/local_output.csv",header=True,inferSchema=True)

df_1

"""#MARKET BASKET ANALYSIS ALGORITHMS

FP-GROWTH ALGORITHM

Apriori algorithm generates all itemsets by scanning the full transactional database. Whereas the FP growth algorithm only generates the frequent itemsets according to the minimum support defined by the user.
FP(Frequent Pattern) Tree is better than Apriori Algorithm.
"""

from pyspark.ml.fpm import FPGrowth
from pyspark.ml.feature import StringIndexer
from pyspark.sql import functions as F

basketdata = df_1.dropDuplicates(['transaction_id','product_name']).sort('transaction_id')
original_data=basketdata
basketdata.show()
print((basketdata.count(), len(basketdata.columns)))

basketdata = basketdata.groupBy("transaction_id").agg(F.collect_list("product_name")).sort('transaction_id')
basketdata.show()

basketdata = basketdata.withColumnRenamed("collect_list(product_name)","Items")
basketdata.show()

fpGrowth = FPGrowth(itemsCol="Items", minSupport=0.006, minConfidence=0.006)
model = fpGrowth.fit(basketdata)

"""minSupport: It sets the minimum support threshold. The minSupport parameter is a fraction between 0 and 1, and it represents the minimum proportion of transactions that must contain a particular pattern (a set of items) for it to be considered frequent. In this case, the threshold is set to 0.006, which means that a pattern must appear in at least 0.6% of the transactions to be considered frequent.


minConfidence: It sets the minimum confidence threshold for generating association rules. The minConfidence parameter is also a fraction between 0 and 1. It represents the minimum level of confidence required for an association rule to be generated. In this case, it's set to 0.006, which corresponds to 0.6% confidence.
"""

model.freqItemsets.show()

from pyspark.sql.functions import desc

sorted_data = model.freqItemsets.sort(desc("freq"))
sorted_data.show()

model.associationRules.show()

"""Association rules describe relationships between items and can be used to understand how frequently items are purchased together and the strength of those associations.

1. antecedent: which are the items typically found together before a purchase.

2. consequent: which are the items that are likely to be purchased if the antecedent items are present.

3. confidence: This is the confidence value, which indicates the probability that the consequent will be purchased when the antecedent is present.

4. lift: The lift value quantifies how much more likely the consequent is to be purchased when the antecedent is present, compared to when the consequent is purchased independently of the antecedent. A lift value greater than 1 indicates a positive association.

5. support: The support count, which represents how frequently the association rule occurs in the dataset.
"""

rules = model.associationRules
model.transform(basketdata).show()

transformed = model.transform(basketdata)
transformed.show()

print((original_data.count(), len(original_data.columns)))
print((transformed.count(), len(transformed.columns)))

import pandas as pd
pandas_df = transformed.toPandas()

pandas_df

import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()

nth_row = 4
row = pandas_df.iloc[nth_row]


items = row['Items']
predictions = row['prediction']


for item in items:
    for prediction in predictions:
        G.add_edge(item, prediction)


pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_size=5, font_size=10, font_color='green', node_color='lightblue')
plt.title(f"Product Associations (Row {nth_row})")
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()

nth_row = 6
row = pandas_df.iloc[nth_row]


items = row['Items']
predictions = row['prediction']


for item in items:
    for prediction in predictions:
        G.add_edge(item, prediction)


pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_size=5, font_size=10, font_color='green', node_color='lightblue')
plt.title(f"Product Associations (Row {nth_row})")
plt.show()

pandas_df.iloc[12,1:]

import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()

nth_row = 12
row = pandas_df.iloc[nth_row]


items = row['Items']
predictions = row['prediction']


for item in items:
    for prediction in predictions:
        G.add_edge(item, prediction)


pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_size=5, font_size=10, font_color='green', node_color='lightblue')
plt.title(f"Product Associations (Row {nth_row})")
plt.show()

rules.show()

"""A lift value greater than 1 suggests a positive association, while a value less than 1 suggests a negative association.

LIFT: The lift value quantifies how much more likely the consequent is to be purchased when the antecedent is present, compared to when the consequent is purchased independently of the antecedent.
"""

min_lift = 1.5

filtered_lift_rules = rules.filter(rules.lift >= min_lift)

count = filtered_lift_rules.count()
print(rules.count())
print("Number of rules greater than the minimum lift: ", count)

""" Confidence - the probability that the a item will be purchased when the other is present.

 since confidence is probability confidence greater than 0.7 would be good
"""

min_confidence = 0.7

filtered_confidence_rules = rules.filter(rules.confidence >= min_confidence)

count = filtered_confidence_rules.count()
print(rules.count())
print("Number of rules greater than the minimum confidence: ", count)

"""support: The support count, which represents how frequently the association rule occurs in the dataset.

Since data set is very large we can take the minimum support to 1%
"""

min_support = 0.01

filtered_support_rules = rules.filter(rules.support >= min_support)

count = filtered_support_rules.count()
print(rules.count())
print("Number of filtered rules based on support:", count)

spark.stop()

