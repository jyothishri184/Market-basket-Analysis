# -*- coding: utf-8 -*-
"""Pre-Processing Market-Basket-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ltE80t-s---XQOUEBN14Dls2HKotmVv4

#MARKET BASKET ANAYSIS

#DATA LOADING
"""

!pip install pyspark

import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MarketBasketAnalysis").getOrCreate()

df_1 = spark.read.csv("/online_retail_II_10_11.csv",header=True,inferSchema=True)

df_1.show()

"""#PREPROCESSING  


DATA EXPLORATION
"""

df_1.describe().show()

df_1.dtypes

"""**Attribute Information:**

**InvoiceNo:** Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.


**StockCode:** Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.


**Description:** Product (item) name. Nominal.


 **Quantity:** The quantities of each product (item) per transaction. Numeric.


 **InvoiceDate:**  Invice date and time. Numeric. The day and time when a transaction was generated.


 **UnitPrice:**  Unit price. Numeric. Product price per unit in sterling (Â£).


 **CustomerID:** Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.


 **Country:**  Country name. Nominal. The name of the country where a customer resides.

COLUMN RENAME
"""

df_1.columns

df_1 = df_1.withColumnRenamed("Invoice","transaction_id")
df_1 = df_1.withColumnRenamed("StockCode","product_id")
df_1 = df_1.withColumnRenamed("Description","product_name")
df_1 = df_1.withColumnRenamed("Quantity","quantity")
df_1 = df_1.withColumnRenamed("InvoiceDate","transaction_date")
df_1 = df_1.withColumnRenamed("Price","price")
df_1 = df_1.withColumnRenamed("Customer ID","customer_id")
df_1 = df_1.withColumnRenamed("Country","country")

df_1.columns

"""DEALING WITH NULL VALUES"""

print((df_1.count(), len(df_1.columns)))

df_1 = df_1.dropna()

print((df_1.count(), len(df_1.columns)))

"""REMOVING DUPLICATES

"""

print((df_1.count(), len(df_1.columns)))

df_1 = df_1.dropDuplicates()

print((df_1.count(), len(df_1.columns)))

"""IN Column 'Transation id' removing rows staring with 'C' SINCE they are cancelled order"""

from pyspark.sql.functions import col

df_1 = df_1.filter(~col("transaction_id").startswith("C"))
df_1.show()

print((df_1.count(), len(df_1.columns)))

"""converting column 'TRANSACTION_ID' to INT"""

df_1 = df_1.withColumn("transaction_id", df_1["transaction_id"].cast("int"))
df_1.show()

df_1.dtypes

"""Spliting 'transaction_date' column to 'transaction_date' and 'transaction_time'"""

df_1 = df_1

df_1.show()

df_1.dtypes

from pyspark.sql.functions import substring,to_date

df_1 = df_1.withColumn('transaction_date', to_date(substring(df_1['transaction_date'], 1, 10), 'dd-MM-yyyy'))

df_1.show()

df_1.dtypes

import os
pandas_df = df_1.toPandas()

local_directory = r"C:\Users\jyoth\Downloads\Projects\MarketBasketAnalysis"

os.makedirs(local_directory, exist_ok=True)

file_name = "local_output.csv"

local_file_path = os.path.join(local_directory, file_name)

pandas_df.to_csv(local_file_path, index=False)

from google.colab import files

files.download("local_output.csv")